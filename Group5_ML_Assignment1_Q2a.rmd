---
title: "ML_Assignment1_Question2a"
author: "Group 5"
date: "26/01/2017"
output: html_document
---

```{r, include=FALSE}
library(dplyr)
options(digits = 4)
```

#### 1. loads the data file;

```{r, include=TRUE}
winequality.white <- read.csv('winequality-white.csv', sep=';')
```

#### 2. construct a new binary column good wine that indicates whether the wine is good (which we define as having a quality of 6 or higher) or not;

```{r, include=TRUE}
winequality.white$good.wine <- ifelse(winequality.white$quality>=6, 1, 0)
#white[12:13] %>% head(5)
```

#### 3. splits the data set into a training data set (~40%), a validation data set (~30%) and a test data set (~30%) and make sure you shuffle the record before the split;

```{r, echo=TRUE}
set.seed(124)
ss <- sample(1:3,size=nrow(winequality.white), replace=TRUE, prob=c(0.4,0.3,0.3))
data.train <- winequality.white[ss==1,]
data.validation <- winequality.white[ss==2,]
data.test <- winequality.white[ss==3,]

data.train.labels <- data.train$good.wine  
data.validation.labels <- data.validation$good.wine
data.test.labels <- data.test$good.wine
```

#### 4. normalises the data according to the Z-score transform;
```{r, echo=TRUE}
data.train.z <- as.data.frame(scale(data.train[1:11])) 
data.validation.z <- as.data.frame(scale(data.validation[1:11]))
data.test.z <- as.data.frame(scale(data.test[1:11]))
```


#### 5. loads and trains the k-Nearest Neighbours classifiers for k = 1, .., 80;
Build KNN classifier:
```{r, echo=TRUE}
library(class)
k_list <- c(1:80)
score <- c()
for (i in k_list) {
kNN <- knn(train = data.train.z, 
            test = data.validation.z, 
            cl = data.train.labels,
            k=i)
#Tabulating the accuracy of each iteration of k(i.e. a match happens for 1-1 or 0-0) which is divided by the total number of records in validation data 
score[i] <- sum(data.validation.labels == kNN)/nrow(data.validation)
}
```

#### 6. evaluates each classifier on the validation set and selects the best classifier;

Print out the scores for each of 80 models.
```{r, echo=FALSE}
for (i in k_list) {
  print(paste('K =', i, ', Accuracy =', score[i]))
}

#Prints accuracy against K-value
plot(score, type="o", col="grey", main = "Accuracy vs K Value", xlab="K Value", ylab = "Accuracy")
text(score, labels=seq(1,80), cex = 0.7, col = "red")
```


```{r, echo=FALSE}
best_k <- which(score == max(score))
print(paste('The best classifier is k =', best_k))
```


#### 7. predicts the generalisation error using the test data set, as well as outputs the result in a confusion matrix.

```{r, echo=TRUE}
library(gmodels)
kNN_test <- knn(train = data.train.z, 
            test = data.test.z, 
            cl = data.train.labels, 
            k=best_k)
CrossTable(x = data.test.labels, y = kNN_test, prop.chisq=FALSE)
score.test <- sum(data.test.labels == kNN_test)/nrow(data.test.z)
options(digits=3)
print(paste('Using the optimal k value of ', best_k, ', we obtain an accuracy of ', round(score.test, digits=4), 'for the test set.'))
print(paste('The generalised error is', 1-round(score.test, digits=4)))
```

#### How do you judge whether the classifier is well-suited for the data set?
One way of determining the suitability of the classifier is to evaluate the accuracy of its prediction on the test data. With an accuracy of 0.7711 for the test data, we can conclude that the classifier is reasonably well suited for the data set.

The optimal k value is determined from iterating the K-nearest-neighbour algorithm for k values of 1 to 80 and choosing the k which yields the highest accuracy for the validation set. The chosen k-value is then used to predict the test set, thus giving an unbiased estimate of the model's performance on future data. However, this approach has its limitations. 

The method we used, also known as the holdout method, depends heavily on which of the data points are included in the training, evaluation and test set. Having too few training data will lead to models in training phase to be of poor quality while too few validation dat will lead to estimates in the validation phase to be of poor quality. Thus the performance of the model is highly dependent on how the division was made. 

A more robust approach would be to use the K-fold cross validation or the leave-one-out cross validation. Either approach will reduce the dependency on how the data is divided while reducing the variance of the resulting estimates. 
