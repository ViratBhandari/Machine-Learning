{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from string import digits\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data into a Python data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                           original filterm\n",
      "0   ham  Go until jurong point, crazy.. Available only ...    None\n",
      "1   ham                      Ok lar... Joking wif u oni...    None\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...    None\n",
      "3   ham  U dun say so early hor... U c already then say...    None\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...    None\n"
     ]
    }
   ],
   "source": [
    "fin = pd.read_table(\"/Users/Chewei/Desktop/UK/ML/smsspamcollection/SMSSpamCollection.csv\", header=None)\n",
    "fin[\"filterm\"] = None\n",
    "fin.columns = [\"label\",\"original\",\"filterm\"] # rename columns to make it clear\n",
    "print(fin.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lower case. (Please provide the Python code that achieves this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                           original  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                             filterm  \n",
      "0  go until jurong point crazy available only in ...  \n",
      "1                            ok lar joking wif u oni  \n",
      "2  free entry in a wkly comp to win fa cup final ...  \n",
      "3        u dun say so early hor u c already then say  \n",
      "4  nah i dont think he goes to usf he lives aroun...  \n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for sentence in fin[\"original\"]:\n",
    "    filtered_sentence =[]\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]','',sentence) #removes all punctuation from sentence\n",
    "    sentence = sentence.translate({ord(k): None for k in digits}) # removes numbers from sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    fin.loc[k,\"filterm\"]=words\n",
    "    k = k+1\n",
    "    \n",
    "for i, w in enumerate(fin.filterm):\n",
    "    w= ' '.join(word for word in fin.filterm[i])\n",
    "    fin.filterm[i]=w\n",
    "    \n",
    "print(fin.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Shuffle the messages and split them into a training set (2,500 messages), a validation set (1,000 messages) and a test set (all remaining messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=(2500/5571), validate_percent=(1000/5571), seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.ix[perm[:train_end]]\n",
    "    validate = df.ix[perm[train_end:validate_end]]\n",
    "    test = df.ix[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, validation, test = train_validate_test_split(fin,seed=19890724)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                           original  \\\n",
      "4962   ham  A bit of Ur smile is my hppnss, a drop of Ur t...   \n",
      "3681   ham  I cant pick the phone right now. Pls send a me...   \n",
      "\n",
      "                                                filterm  \n",
      "4962  a bit of ur smile is my hppnss a drop of ur te...  \n",
      "3681  i cant pick the phone right now pls send a mes...  \n",
      "     label                                           original  \\\n",
      "2845   ham  Today iZ Yellow rose day. If u love my frndshi...   \n",
      "2611   ham     As usual..iam fine, happy &amp; doing well..:)   \n",
      "\n",
      "                                                filterm  \n",
      "2845  today iz yellow rose day if u love my frndship...  \n",
      "2611              as usualiam fine happy amp doing well  \n",
      "     label                                           original  \\\n",
      "2273   ham  Haha awesome, I've been to 4u a couple times. ...   \n",
      "1701   ham                    Please ask mummy to call father   \n",
      "\n",
      "                                                filterm  \n",
      "2273  haha awesome ive been to u a couple times who ...  \n",
      "1701                    please ask mummy to call father  \n"
     ]
    }
   ],
   "source": [
    "print(train.head(2))\n",
    "print(validation.head(2))\n",
    "print(test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explain the code: What is the purpose of each function? What do ’train’ and ‘train2’ do, and what is the difference between them? Where in the code is Bayes’ Theorem being applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam :\n",
    "    def train ( self , hamMessages , spamMessages) : \n",
    "        self.words = set ('_'.join (hamMessages + spamMessages).split()) #Create a set consisting of \n",
    "        #all the words that occur in the mails\n",
    "        self.priors = np.zeros (2) # probability of being spam / ham without further knowledge\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len ( spamMessages ) )\n",
    "        # the above one is P(ham)\n",
    "        self.priors [1] = 1.0 - self.priors[0]  # P(spam)\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len ( hamMessages )\n",
    "            # if word of \"words\" is in hamMessages, then add one to the numerator, after looping\n",
    "            # this number will be divided by the total word of hamMessages\n",
    "            # = P(word|ham)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) /len ( spamMessages )\n",
    "            # if word of \"words\" is in spamMessages, then add one to the numerator, after looping\n",
    "            # this number will be divided by the total word of spamMessages\n",
    "            # = P(word|spam)\n",
    "            self.likelihoods.append ([min (prob1 , 0.95) , min (prob2 ,0.95) ])\n",
    "            # when some words do not appear in ham or spam, the probability will be 1. That is the probability \n",
    "            #of occurance is 0. To avoid a 0 divisor we the Laplace tranformation is applied.\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "            # just transpose the dataframe for further use\n",
    "        \n",
    "    \n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        self.words = set(''.join(list(hamMessages) + list(spamMessages)).split()) #Create a set consisting of \n",
    "        #all the words that occur in the mails\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages)) #P(ham) \n",
    "        self.priors[1] = 1.0 - self.priors[0] #P(spam)\n",
    "        self.likelihoods = []\n",
    "        spamkeywords = [ ]\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages) #P(W|ham)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages)  #P(W|spam)\n",
    "            if prob1 * 20 < prob2:\n",
    "                # if probability of a word being spam is 20 times higher than being ham (that is, 20 times more\n",
    "                #more likely to appear in spam than in ham)\n",
    "                #then this word will be taken into the dataframe of spamkeywords\n",
    "                self.likelihoods.append([min(prob1 , 0.95) , min(prob2 , 0.95)]) #Laplace transformation\n",
    "                spamkeywords.append(w) \n",
    "        self.words = spamkeywords\n",
    "                # update the words using only highly (filtered) spam words\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "                # just transpose the dataframe for further use\n",
    "        \n",
    "    def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "            # copy P(ham), P(spam)\n",
    "        for i, w in enumerate(self.words):\n",
    "            # using words in train one, using spam keywords in train two\n",
    "            if w in message.lower(): \n",
    "                # convert to lower−case\n",
    "                posteriors *= self.likelihoods[:,i] \n",
    "                # using bayes' theorem and conditional independence here\n",
    "                # P(viagara,paypal,now | spam) is propotional to \n",
    "                #              P(viagara|spam)*P(paypal|spam)*P(now|spam)*P(spam)\n",
    "                # the first three probability is in self.likehoods, and the last one is in posteriors\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:,i] \n",
    "                # if the word is not in the message\n",
    "                # then the posteriors multiply P(not the word|spam ) or P(not the word|ham)\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors, ord = 1) \n",
    "                #normalise using max(sum(abs(x), axis=0))\n",
    "        if posteriors[0] > 0.5:\n",
    "            # set the cutoff value  0.5\n",
    "            # if probability of being ham is greater than 0.5, them we say this is a ham mail\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]] \n",
    "\n",
    "    def score(self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape(2, 2) \n",
    "        num=0\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham': \n",
    "                confusion[0 ,0] += 1\n",
    "            # if we predict the message is ham and it is actually ham, the +1 in the true positive \n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam': \n",
    "                confusion[0 ,1] += 1\n",
    "            # if we predict the message is ham and it is actually spam, the +1 in the false negative \n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham': \n",
    "                confusion[1 ,0] += 1\n",
    "            # if we predict the message is spam and it is actually ham, the +1 in the false positive\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam': \n",
    "                confusion[1 ,1] += 1\n",
    "            # if we predict the message is ham and it is actually ham, the +1 in the true negative \n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion\n",
    "            # add up the diagonal figures and divide by total numbers of cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use your training set to train the classifiers ‘train’ and ‘train2’. Note that the interfaces of our classifiers require you to pass the ham and spam messages separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since the train function will take spam and ham separately, here we are going to sparate these two kinds of messages \n",
    "train_spam = train[train.label==\"spam\"][\"filterm\"]\n",
    "train_ham = train[train.label==\"ham\"][\"filterm\"]\n",
    "# Creating the list of strings to be used for training\n",
    "train_spam  = list(itertools.chain(train_spam))\n",
    "train_ham = list(itertools.chain(train_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following is the accuracy rate based on train set. The accurancy on validation set in next paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SpamFilter = NaiveBayesForSpam() # initiate the function\n",
    "SpamFilter.train(train_ham,train_spam)  \n",
    "score, cm = SpamFilter.score(train.filterm, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.972\n",
      "\n",
      "confusion matrix:\n",
      "[[ 2139.    39.]\n",
      " [   31.   291.]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', score)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train2\n",
    "SpamFilter = NaiveBayesForSpam()\n",
    "SpamFilter.train2(train_ham,train_spam)\n",
    "score2, cm2 = SpamFilter.score(train.filterm, train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9724\n",
      "\n",
      "confusion matrix:\n",
      "[[ 2163.    60.]\n",
      " [    9.   268.]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', score2)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using the validation set, explore how each of the two classifiers performs out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_vd, cm_vd = SpamFilter.score(validation.filterm, validation.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.956\n",
      "\n",
      "confusion matrix:\n",
      "[[ 843.   34.]\n",
      " [  10.  113.]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', score_vd)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm_vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SpamFilter.train2(train_ham,train_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_vd2, cm_vd2 = SpamFilter.score(validation.filterm, validation.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.959\n",
      "\n",
      "confusion matrix:\n",
      "[[ 852.   40.]\n",
      " [   1.  107.]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', score_vd2)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm_vd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for train() and train2() for validation set is 0.956 and 0.959 respectively. The accuracy for train2() is higher. train() may have resulted in overfitting, hence resulting in lower accuracy for validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Why is the ‘train2’ classifier faster? Why does it yield a better accuracy both on the training and the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including a marker (print(i)) for the number of iterations in the predict() function, we can see that there is a significant reduction in the number of iterations when we use train2() instead of train()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "        for i, w in enumerate(self.words):\n",
    "            if w in message.lower():\n",
    "                posteriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors)\n",
    "            print(i) # prints the number of iterations\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the message \"Money Viagra\" as an input to predict() function, there are 5501 iterations when using train() and 172 iterations when using train2(). Fewer iterations are achieved because train2() stores only the spam key words for later comparison with new records as compared to storing both ham and spam words encountered in the training set. Therefore, train2 classifier is fast.\n",
    "\n",
    "train2() is likely to yield a better accuracy for both the training and the validation set. This is because train2() assigns a word to spamkeywords list when prob1*20 < prob2.  The value of 20 in prob1 *20 < prob2 determines the accuracy of the spamkeywords list. In turn, the accuracy of the list of spam key words will then impact the accuracy of new records being accurately classified as spam or ham. \n",
    "\n",
    "However, it should be noted that train2() uses less predictors(i.e. spamkeywords) than train(), the accuracy may not always be higher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How many false positives (ham messages classified as spam messages) did you get in your validation set? How would you change the code to reduce false positives at the expense of possibly having more false negatives (spam messages classified as ham messages)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 'train' classifier, the false positive is 34 and the false positive of train two is 49. The reason is that train two only tests those highly-spam keyword and thus has higher accuarcy. \n",
    "\n",
    "To reduce the false positives at the expense of having more false negatives, we can adjust the threshold of a message being classified as spam from 0.50 to 0.70. This adjustment requires a message to have more spam words before being classified as a spam message. The accuracy after adjusting the threshold is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam_edited :\n",
    "    def train ( self , hamMessages , spamMessages) : \n",
    "        self.words = set ('_'.join (hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2) # probability of being spam / ham without further knowledge\n",
    "        self.priors[0] = float (len (hamMessages)) / (len (hamMessages) + len ( spamMessages ) )\n",
    "        # the above one is P(ham)\n",
    "        self.priors [1] = 1.0 - self.priors[0]  # P(spam)\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len ( hamMessages )\n",
    "            # if word of \"words\" is in hamMessages, then add one to the numerator, after looping\n",
    "            # this number will be divided by the total word of hamMessages\n",
    "            # = P(word|ham)\n",
    "            prob2 = (1.0 + len ([m for m in spamMessages if w in m])) /len ( spamMessages )\n",
    "            # if word of \"words\" is in spamMessages, then add one to the numerator, after looping\n",
    "            # this number will be divided by the total word of spamMessages\n",
    "            # = P(word|spam)\n",
    "            self.likelihoods.append ([min (prob1 , 0.95) , min (prob2 ,0.95) ])\n",
    "            # when some words do not appear in ham or spam, the probability will be 1. The above line\n",
    "            # avoid this situation\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "            # just transpose the dataframe for further use\n",
    "        \n",
    "    \n",
    "    def train2(self, hamMessages, spamMessages):\n",
    "        self.words = set(''.join(list(hamMessages) + list(spamMessages)).split())\n",
    "        self.priors = np.zeros(2)\n",
    "        self.priors[0] = float(len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0] \n",
    "        self.likelihoods = []\n",
    "        spamkeywords = [ ]\n",
    "        for i, w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages) \n",
    "            if prob1 * 20 < prob2:\n",
    "                self.likelihoods.append([min(prob1 , 0.95) , min(prob2 , 0.95)])\n",
    "                spamkeywords.append(w) \n",
    "                # if probability of a word being spam is 20 times higher tham being ham\n",
    "                # then this word will be taken into the dataframe of spamkeywords\n",
    "        self.words = spamkeywords\n",
    "                # update the words unsing only highly spam words\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "                # just transpose the dataframe for further use\n",
    "        \n",
    "    def predict(self, message):\n",
    "        posteriors = np.copy(self.priors)\n",
    "            # copy P(ham), P(spam)\n",
    "        for i, w in enumerate(self.words):\n",
    "            # using words in train one, using spam keywords in train two\n",
    "            if w in message.lower(): \n",
    "                # convert to lower−case\n",
    "                posteriors *= self.likelihoods[:,i] \n",
    "                # using bayes' theorem here\n",
    "                # P(viagara,paypal,now | spam) is propotional to \n",
    "                #              P(viagara|spam)*P(paypal|spam)*P(now|spam)*P(spam)\n",
    "                # the first three probability is in self.likehoods, and the last one is in posteriors\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:,i] \n",
    "                # if the word is not in the message\n",
    "                # then the posteriors multipy P(not the word|spam ) or P(not the word|ham)\n",
    "            posteriors = posteriors / np.linalg.norm(posteriors, ord = 1) \n",
    "                #normalise using max(sum(abs(x), axis=0))\n",
    "        if posteriors[0] > 0.7:\n",
    "            # set the cutoff value  0.5\n",
    "            # if probability of being ham is greater than 0.5, them we say this is a ham mail\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]] \n",
    "\n",
    "    def score(self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape(2, 2) \n",
    "        num=0\n",
    "        for m, l in zip(messages, labels):\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham': \n",
    "                confusion[0 ,0] += 1\n",
    "            # if the we predict the message is ham and it is actually ham, the +1 in the true positive \n",
    "            elif self.predict(m)[0] == 'ham' and l == 'spam': \n",
    "                confusion[0 ,1] += 1\n",
    "            # if the we predict the message is ham and it is actually spam, the +1 in the false negative \n",
    "            elif self.predict(m)[0] == 'spam' and l == 'ham': \n",
    "                confusion[1 ,0] += 1\n",
    "            # if the we predict the message is spam and it is actually ham, the +1 in the false positive\n",
    "            elif self.predict(m)[0] == 'spam' and l == 'spam': \n",
    "                confusion[1 ,1] += 1\n",
    "            # if the we predict the message is ham and it is actually ham, the +1 in the true negative \n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion\n",
    "            # add up the diagonal figures and divide by total numbers of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SpamFilter_edited = NaiveBayesForSpam_edited ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.965\n",
      "\n",
      "confusion matrix:\n",
      "[[ 841.   23.]\n",
      " [  12.  124.]]\n"
     ]
    }
   ],
   "source": [
    "SpamFilter_edited.train(train_ham,train_spam)\n",
    "score_p1, cm_p1 = SpamFilter_edited.score(validation.filterm, validation.label)\n",
    "print('accuracy:', score_p1)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm_p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original false positive is 34, and the improved version is 23. But the false negative increases from 10 to 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n",
      "\n",
      "confusion matrix:\n",
      "[[ 852.   39.]\n",
      " [   1.  108.]]\n"
     ]
    }
   ],
   "source": [
    "SpamFilter_edited.train2(train_ham,train_spam)\n",
    "score_p2, cm_p2 = SpamFilter.score(validation.filterm, validation.label)\n",
    "print('accuracy:', score_p2)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is a slight improvement for train2(). The original false positive is 40, and the improved version is 39. False negative reamins as 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Run the ‘train2’ classifier on the test set and report its performance using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.97249034749\n",
      "\n",
      "confusion matrix:\n",
      "[[ 1794.    51.]\n",
      " [    6.   221.]]\n"
     ]
    }
   ],
   "source": [
    "SpamFilter.train2(train_ham,train_spam)\n",
    "score_test, cm_test = SpamFilter.score(test.filterm, test.label)\n",
    "print('accuracy:', score_test)\n",
    "print()\n",
    "print('confusion matrix:')\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The accuracy on test set is 97.249%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
